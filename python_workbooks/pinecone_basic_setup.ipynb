{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RTjQBESc9Io9"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pinecone-client\n",
        "!pip install langchain\n",
        "!pip install -U langchain-community\n",
        "!pip install pypdf\n",
        "!pip install jq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pdfplumber\n",
        "import pinecone\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader, JSONLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pypdf"
      ],
      "metadata": {
        "id": "m3sdv6d49Vdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to Pinecone, sign up. Copy the API key below.\n",
        "\n",
        "Under Database->Indexes, create an index with Dimensions: 1024 and Metric: cosine. You can do this by selecting multilingual-e5-large which has the right settings for this example implementation.\n",
        "\n",
        "If you forgot to copy your default API Key, you can create a new one.\n",
        "\n",
        "Put the details here:"
      ],
      "metadata": {
        "id": "Dg3G06ZQ9dTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = \"<your-pinecone-api-key>\"\n",
        "\n",
        "index_name = \"<your-index-name>\"\n",
        "index_host = \"<your-index-host>\""
      ],
      "metadata": {
        "id": "xgJXHH7c9Y6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pc = pinecone.Pinecone(\n",
        "        api_key=pinecone_api_key\n",
        ")\n",
        "\n",
        "# Instantiate the index\n",
        "index = pinecone.Index(name=index_name, host=index_host, api_key=pinecone_api_key)\n",
        "namespace = \"<choose-a-name>\""
      ],
      "metadata": {
        "id": "2FK8kzri-luo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with the chunk size and the chunk overlap of your data (in tokens)\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 20\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Replace consecutive spaces, newlines and tabs\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "def load_txt(file_path):\n",
        "    loader = TextLoader(file_path)\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "def load_json(file_path, json_schema):\n",
        "    loader = JSONLoader(file_path, jq_schema=json_schema)\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "def process_texts(raw_texts):\n",
        "    # Split your data up into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "    documents = text_splitter.split_documents(raw_texts)\n",
        "    # Convert Document objects into strings\n",
        "    texts = [{'page_content': doc.page_content, 'meta_data': doc.metadata} for doc in documents]\n",
        "    return texts\n",
        "\n",
        "# Define a function to create embeddings\n",
        "def create_embeddings_pinecone(texts):\n",
        "  embeddings_list = []\n",
        "  for text in texts:\n",
        "    a = pc.inference.embed(\n",
        "      model=\"multilingual-e5-large\",\n",
        "      inputs=[text['page_content']],\n",
        "      parameters={\n",
        "          \"input_type\": \"passage\",\n",
        "          \"truncate\": \"END\"\n",
        "      }\n",
        "    )\n",
        "    embeddings_list.append(a)\n",
        "  return embeddings_list"
      ],
      "metadata": {
        "id": "TcKNEf4w-zHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "texts_raw = []"
      ],
      "metadata": {
        "id": "RhIQXuQJ--VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your files must be in your google drive in a directory called \"RAG data\"\n"
      ],
      "metadata": {
        "id": "p9wPGM9w_Mem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all pdf documents (skip if you don't have any pdfs)\n",
        "pdf_files = ['example1.pdf',\n",
        "             'example2.pdf'\n",
        "             ]\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "  texts_raw.extend(load_pdf(\"/content/drive/MyDrive/RAG data/\"+pdf_file))\n",
        "\n",
        "print(type(texts_raw))\n",
        "print(len(texts_raw))\n",
        "print(texts_raw[-1])"
      ],
      "metadata": {
        "id": "C8NKCs8l_COE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all text documents (skip if you don't have any txts)\n",
        "txt_files = ['example1.txt',\n",
        "             'example2.txt'\n",
        "             ]\n",
        "\n",
        "for txt_file in txt_files:\n",
        "  texts_raw.extend(load_txt(\"/content/drive/MyDrive/RAG data/\"+txt_file))\n",
        "\n",
        "print(type(texts_raw))\n",
        "print(len(texts_raw))\n",
        "print(texts_raw[-1])"
      ],
      "metadata": {
        "id": "Yz6PQjYC8e0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all json documents. You need to provide a jq_schema for these as well, which\n",
        "# indicates where to find the content (skip if you don't have any jsons)\n",
        "json_files = [['example1.json', '[].content'],\n",
        "              ['example2.json', '[].text']\n",
        "             ]\n",
        "\n",
        "for json_file in json_files:\n",
        "  texts_raw.extend(load_txt(\"/content/drive/MyDrive/RAG data/\"+json_file[0], json_file[1]))\n",
        "\n",
        "print(type(texts_raw))\n",
        "print(len(texts_raw))\n",
        "print(texts_raw[-1])"
      ],
      "metadata": {
        "id": "MBthIEyIG6kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = process_texts(texts_raw)\n",
        "print(len(texts))\n",
        "print(texts[0])"
      ],
      "metadata": {
        "id": "BA8xtf388sXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings\n",
        "embeddings = create_embeddings_pinecone(texts)\n",
        "len(embeddings) # should match the last len(texts)"
      ],
      "metadata": {
        "id": "9yO6D7ek_WiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete all first -- only run this if you want to clear out the namespace to add all embeddings from scratch\n",
        "# will error if it doesn't exist yet -- not a problem!\n",
        "index.delete(delete_all=True, namespace=namespace)"
      ],
      "metadata": {
        "id": "2YHBDcvi_hH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsert the vectors and text to Pinecone\n",
        "vectors = []\n",
        "for text, embedding in zip(texts, embeddings):\n",
        "    if text is not None:\n",
        "      vectors.append({\n",
        "          \"id\": str(len(vectors)),\n",
        "          \"values\": embedding[0]['values'],\n",
        "          \"metadata\": {'text': text['page_content']}\n",
        "      })\n",
        "\n",
        "print(len(vectors)) # should match the len(embeddings above)\n",
        "\n",
        "# This may take a while! Approx 1 minute per 500 vectors. Wait for the 'Done!'\n",
        "for vector in vectors:\n",
        "  index.upsert(\n",
        "      vectors=[vector],\n",
        "      namespace=namespace\n",
        "  )\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "y5WWZXnF_qTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now go to iostack.\n",
        "\n",
        "In your Account, go to 'Secrets & Access Keys', and add a new one by clicking on the '+'. The type should be 'Pinecone RAG Access Key'. The Key is your Pinecone API key you used above.\n",
        "\n",
        "In your Agent, under Integrations, set up a new Pinecone integration:\n",
        "\n",
        "Pinecone Index Name: {index_name} (above)\n",
        "\n",
        "Access Key: the one you just set up\n",
        "\n",
        "Namespace: {namespace} (above)\n",
        "\n",
        "Text Field Name: 'text'\n",
        "\n",
        "Then, from either the Common Stage Settings or the Stage/s in the flow where you want to use this RAG service, you can add this under Integrations."
      ],
      "metadata": {
        "id": "FYTIMNnl_5y7"
      }
    }
  ]
}